{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c913e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:52:28) \\n[GCC 9.4.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a84f57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in ./anaconda3/lib/python3.8/site-packages (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in ./anaconda3/lib/python3.8/site-packages (from mxnet) (1.20.3)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in ./anaconda3/lib/python3.8/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in ./anaconda3/lib/python3.8/site-packages (from mxnet) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
      "Requirement already satisfied: gluonnlp==0.8.0 in ./anaconda3/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.8/site-packages (from gluonnlp==0.8.0) (1.20.3)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.8/site-packages (1.3.3)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/lib/python3.8/site-packages (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./anaconda3/lib/python3.8/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./anaconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./anaconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in ./anaconda3/lib/python3.8/site-packages (0.1.91)\n",
      "Requirement already satisfied: transformers==4.8.2 in ./anaconda3/lib/python3.8/site-packages (4.8.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (4.62.3)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (0.0.53)\n",
      "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (0.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (0.10.3)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (2021.9.30)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (1.20.3)\n",
      "Requirement already satisfied: packaging in ./anaconda3/lib/python3.8/site-packages (from transformers==4.8.2) (21.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers==4.8.2) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.8/site-packages (from packaging->transformers==4.8.2) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers==4.8.2) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers==4.8.2) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers==4.8.2) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers==4.8.2) (1.26.7)\n",
      "Requirement already satisfied: six in ./anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.8.2) (1.16.0)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.8.2) (8.0.1)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==4.8.2) (1.0.1)\n",
      "Requirement already satisfied: torch in ./anaconda3/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.8/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.8/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./anaconda3/lib/python3.8/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.8/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./anaconda3/lib/python3.8/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./anaconda3/lib/python3.8/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.8/site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./anaconda3/lib/python3.8/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.8/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./anaconda3/lib/python3.8/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./anaconda3/lib/python3.8/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: wheel in ./anaconda3/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.0.4)\n",
      "Requirement already satisfied: lit in ./anaconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
      "Requirement already satisfied: cmake in ./anaconda3/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.8/site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp==0.8.0\n",
    "!pip install pandas tqdm   \n",
    "!pip install sentencepiece==0.1.91\n",
    "!pip install transformers==4.8.2\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0395e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-71jm_g80/kobert-tokenizer_4de0280c0eba48a98440efdea3d74e88\n",
      "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-71jm_g80/kobert-tokenizer_4de0280c0eba48a98440efdea3d74e88\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n"
     ]
    }
   ],
   "source": [
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5db0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihwan/anaconda3/lib/python3.8/site-packages/mxnet/optimizer/optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n",
      "2023-05-22 12:43:01.552541: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "\n",
    "#GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70f999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "#vocab = tokenizer.get_vocab()\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "#model, vocab = BertModel.from_pretrained('skt/kobert-base-v1', tokenizer.vocab_file)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c96d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('/home/jihwan/MinJeong Archive/감정분류데이터셋.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97366d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>공포</th>\n",
       "      <th>5468</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>혹시 유산인 가능성도 잇나요...?</td>\n",
       "      <td>공포</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16680</th>\n",
       "      <td>무슨 은행이여 시발</td>\n",
       "      <td>분노</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35985</th>\n",
       "      <td>사진찍어서 숫자 파악하는거 봐서 저 경찰청장도 나 앉아야 겟구만. ㅎㅎㅎ</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7340</th>\n",
       "      <td>다들 그만 두셧남...?</td>\n",
       "      <td>놀람</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36064</th>\n",
       "      <td>노무현 대통령과 당신은 많이 다르다</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31329</th>\n",
       "      <td>재미따 재미따 재미따 ㅎㅎㅎㅎ</td>\n",
       "      <td>행복</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37001</th>\n",
       "      <td>언제 터질지 모르는거 옆에 있으면.</td>\n",
       "      <td>혐오</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19957</th>\n",
       "      <td>거기에 집대출까지 끼면 고충이 크겠지.</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19347</th>\n",
       "      <td>난 고영태가 부럽다...</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18338</th>\n",
       "      <td>너무 안가리면 욕먹을까봐 좀가렸더니 더욕먹었네요ㅜ</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Sentence Emotion  Unnamed: 2  \\\n",
       "2855                        혹시 유산인 가능성도 잇나요...?      공포         NaN   \n",
       "16680                                무슨 은행이여 시발      분노         NaN   \n",
       "35985  사진찍어서 숫자 파악하는거 봐서 저 경찰청장도 나 앉아야 겟구만. ㅎㅎㅎ      혐오         NaN   \n",
       "7340                              다들 그만 두셧남...?      놀람         NaN   \n",
       "36064                       노무현 대통령과 당신은 많이 다르다      혐오         NaN   \n",
       "31329                          재미따 재미따 재미따 ㅎㅎㅎㅎ      행복         NaN   \n",
       "37001                       언제 터질지 모르는거 옆에 있으면.      혐오         NaN   \n",
       "19957                     거기에 집대출까지 끼면 고충이 크겠지.      슬픔         NaN   \n",
       "19347                             난 고영태가 부럽다...      슬픔         NaN   \n",
       "18338               너무 안가리면 욕먹을까봐 좀가렸더니 더욕먹었네요ㅜ      슬픔         NaN   \n",
       "\n",
       "       Unnamed: 3  Unnamed: 4   공포  5468  \n",
       "2855          NaN         NaN  NaN   NaN  \n",
       "16680         NaN         NaN  NaN   NaN  \n",
       "35985         NaN         NaN  NaN   NaN  \n",
       "7340          NaN         NaN  NaN   NaN  \n",
       "36064         NaN         NaN  NaN   NaN  \n",
       "31329         NaN         NaN  NaN   NaN  \n",
       "37001         NaN         NaN  NaN   NaN  \n",
       "19957         NaN         NaN  NaN   NaN  \n",
       "19347         NaN         NaN  NaN   NaN  \n",
       "18338         NaN         NaN  NaN   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78f696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
    "data.loc[(data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
    "data.loc[(data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
    "data.loc[(data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
    "data.loc[(data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
    "data.loc[(data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
    "data.loc[(data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6\n",
    "\n",
    "data_list = []\n",
    "for ques, label in zip(data['Sentence'], data['Emotion'])  :\n",
    "    data = []   \n",
    "    data.append(ques)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7655f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentenceTransform:\n",
    "    \"\"\"BERT style data transformation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : BERTTokenizer.\n",
    "        Tokenizer for the sentences.\n",
    "    max_seq_length : int.\n",
    "        Maximum sequence length of the sentences.\n",
    "    pad : bool, default True\n",
    "        Whether to pad the sentences to maximum length.\n",
    "    pair : bool, default True\n",
    "        Whether to transform sentences or sentence pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._pad = pad\n",
    "        self._pair = pair\n",
    "        self._vocab = vocab \n",
    "\n",
    "    def __call__(self, line):\n",
    "\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer.tokenize(text_a)\n",
    "        tokens_b = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
    "                                    self._max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
    "        # pre-training and are added to the wordpiece embedding vector\n",
    "        # (and position vector). This is not *strictly* necessary since\n",
    "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        #vocab = self._tokenizer.vocab\n",
    "        vocab = self._vocab\n",
    "        tokens = []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The valid length of sentences. Only real  tokens are attended to.\n",
    "        valid_length = len(input_ids)\n",
    "\n",
    "        if self._pad:\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            # use padding tokens for the rest\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ba3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        #transform = nlp.data.BERTSentenceTransform(\n",
    "        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8848c77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class BERTDataset(Dataset):\\n    #def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len, pad, pair):\\n    def __init__(self, dataset, sent_idx, label_idx, tokenizer, max_len):\\n        self.tokenizer = tokenizer\\n        self.sentences = [tokenizer.encode_plus(i[sent_idx], max_length=max_len, padding='max_length', truncation=True) for i in dataset]\\n        self.labels = [np.int32(i[label_idx]) for i in dataset]\\n\\n    def __getitem__(self, i):\\n        return (torch.tensor(self.sentences[i]['input_ids']), torch.tensor(self.sentences[i]['attention_mask']), self.labels[i])\\n    \\n    def __len__(self):\\n        return (len(self.labels))\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class BERTDataset(Dataset):\n",
    "    #def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len, pad, pair):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = [tokenizer.encode_plus(i[sent_idx], max_length=max_len, padding='max_length', truncation=True) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (torch.tensor(self.sentences[i]['input_ids']), torch.tensor(self.sentences[i]['attention_mask']), self.labels[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f85837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187e822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c46082be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n",
    "\n",
    "#저는 csv 파일을 사용하여 이부분은 dataset_train.values 로 해주었습니다. \n",
    "data_train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f1d693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0684a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf762f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        #train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e1e65b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f6ee7830640>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badcdcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5495/1756086221.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c4df6607d64c67930966aadf895db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.087026834487915 train acc 0.125\n",
      "epoch 1 batch id 201 loss 2.009521961212158 train acc 0.1837686567164179\n",
      "epoch 1 batch id 401 loss 1.7191959619522095 train acc 0.24594763092269326\n",
      "epoch 1 batch id 601 loss 1.336506962776184 train acc 0.3011647254575707\n",
      "epoch 1 batch id 801 loss 1.4594199657440186 train acc 0.33224094881398253\n",
      "epoch 1 batch id 1001 loss 1.3453142642974854 train acc 0.36126373626373626\n",
      "epoch 1 batch id 1201 loss 1.5038388967514038 train acc 0.37848667776852624\n",
      "epoch 1 batch id 1401 loss 1.0467135906219482 train acc 0.3957887223411849\n",
      "epoch 1 batch id 1601 loss 1.2182133197784424 train acc 0.40517645221736415\n",
      "epoch 1 batch id 1801 loss 1.145581603050232 train acc 0.41390199888950585\n",
      "epoch 1 train acc 0.4193741168158267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5495/1756086221.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d530276e121c48f2b2e61952c047833a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.49846569062407575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39ed70bf4f1454bafa8be04e6e80f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.6104506254196167 train acc 0.375\n",
      "epoch 2 batch id 201 loss 1.538264274597168 train acc 0.49409203980099503\n",
      "epoch 2 batch id 401 loss 1.2234630584716797 train acc 0.4959476309226933\n",
      "epoch 2 batch id 601 loss 1.207266092300415 train acc 0.5064475873544093\n",
      "epoch 2 batch id 801 loss 1.2728122472763062 train acc 0.5205212234706617\n",
      "epoch 2 batch id 1001 loss 1.2259862422943115 train acc 0.5332167832167832\n",
      "epoch 2 batch id 1201 loss 1.0703492164611816 train acc 0.5415278934221482\n",
      "epoch 2 batch id 1401 loss 0.7758867740631104 train acc 0.5521948608137045\n",
      "epoch 2 batch id 1601 loss 0.831831693649292 train acc 0.5582058088694566\n",
      "epoch 2 batch id 1801 loss 0.7351546883583069 train acc 0.5624305941143809\n",
      "epoch 2 train acc 0.566150494583137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbde01fb251e42b1b6af05ffdbe8b8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.5278393966282166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bbaa1b3f944b3ba99783d96cb8c0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 1.726820468902588 train acc 0.4375\n",
      "epoch 3 batch id 201 loss 0.9363678097724915 train acc 0.6228233830845771\n",
      "epoch 3 batch id 401 loss 0.6747530698776245 train acc 0.6234413965087282\n",
      "epoch 3 batch id 601 loss 0.9961006045341492 train acc 0.6324875207986689\n",
      "epoch 3 batch id 801 loss 0.9291415214538574 train acc 0.6403714107365793\n",
      "epoch 3 batch id 1001 loss 0.7503542900085449 train acc 0.6517857142857143\n",
      "epoch 3 batch id 1201 loss 0.6323425769805908 train acc 0.6602310574521232\n",
      "epoch 3 batch id 1401 loss 0.6211060881614685 train acc 0.66947715917202\n",
      "epoch 3 batch id 1601 loss 0.6783206462860107 train acc 0.6750078076202374\n",
      "epoch 3 batch id 1801 loss 0.26468732953071594 train acc 0.6793101054969461\n",
      "epoch 3 train acc 0.6824364107395196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2158716500fe4d918b1d36a494b44c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.540391156462585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368d02dbfbd14797a235d0b30c84b4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 1.1512116193771362 train acc 0.5625\n",
      "epoch 4 batch id 201 loss 0.852821409702301 train acc 0.7450248756218906\n",
      "epoch 4 batch id 401 loss 0.3801771402359009 train acc 0.7434538653366584\n",
      "epoch 4 batch id 601 loss 0.7560967803001404 train acc 0.7485440931780366\n",
      "epoch 4 batch id 801 loss 0.7512058615684509 train acc 0.7521847690387016\n",
      "epoch 4 batch id 1001 loss 0.4942791759967804 train acc 0.7618631368631369\n",
      "epoch 4 batch id 1201 loss 0.47156521677970886 train acc 0.7688384679433805\n",
      "epoch 4 batch id 1401 loss 0.5357431173324585 train acc 0.7758297644539615\n",
      "epoch 4 batch id 1601 loss 0.4213167428970337 train acc 0.7800983760149907\n",
      "epoch 4 batch id 1801 loss 0.16817137598991394 train acc 0.7829677956690727\n",
      "epoch 4 train acc 0.7852213848327838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e7ac6fe9a4631a31987895aead2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.5444395149364094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e00ae8d8d14497be00aa3670145a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.833455502986908 train acc 0.6875\n",
      "epoch 5 batch id 201 loss 0.6113192439079285 train acc 0.8274253731343284\n",
      "epoch 5 batch id 401 loss 0.41248688101768494 train acc 0.8271508728179551\n",
      "epoch 5 batch id 601 loss 0.7295897006988525 train acc 0.8338186356073212\n",
      "epoch 5 batch id 801 loss 0.6163555383682251 train acc 0.8347378277153558\n",
      "epoch 5 batch id 1001 loss 0.25137859582901 train acc 0.8408466533466533\n",
      "epoch 5 batch id 1201 loss 0.28127631545066833 train acc 0.8449729392173189\n",
      "epoch 5 batch id 1401 loss 0.4076000452041626 train acc 0.8480995717344754\n",
      "epoch 5 batch id 1601 loss 0.2557951509952545 train acc 0.8497813866333541\n",
      "epoch 5 batch id 1801 loss 0.0762912780046463 train acc 0.8501179900055524\n",
      "epoch 5 train acc 0.851233513895431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df9402f8205416cbe02cd9d54ba4251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.5458629103815439\n"
     ]
    }
   ],
   "source": [
    "train_history=[]\n",
    "test_history=[]\n",
    "loss_history=[]\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "         \n",
    "        #print(label.shape,out.shape)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "            train_history.append(train_acc / (batch_id+1))\n",
    "            loss_history.append(loss.data.cpu().numpy())\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    #train_history.append(train_acc / (batch_id+1))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    test_history.append(test_acc / (batch_id+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d1e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"공포가\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"놀람이\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                test_eval.append(\"분노가\")\n",
    "            elif np.argmax(logits) == 3:\n",
    "                test_eval.append(\"슬픔이\")\n",
    "            elif np.argmax(logits) == 4:\n",
    "                test_eval.append(\"중립이\")\n",
    "            elif np.argmax(logits) == 5:\n",
    "                test_eval.append(\"행복이\")\n",
    "            elif np.argmax(logits) == 6:\n",
    "                test_eval.append(\"혐오가\")\n",
    "\n",
    "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e20bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하고싶은 말을 입력해주세요 : 오늘은 정말 기분이 좋아요!\n",
      ">> 입력하신 내용에서 행복이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이런 일이 또 생기면 화가 나요!\n",
      ">> 입력하신 내용에서 분노가 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이런 일을 절대로 다시 경험하고 싶지 않아요!\n",
      ">> 입력하신 내용에서 분노가 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 어둠 속에서 무서운 소리가 들려와서 겁이 났어요.\n",
      ">> 입력하신 내용에서 공포가 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 오늘 날씨가 좋네요\n",
      ">> 입력하신 내용에서 행복이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이별이 너무 슬퍼서 마음이 아파요\n",
      ">> 입력하신 내용에서 슬픔이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 정말 놀랐어요! 이런 일이 일어날 줄은 몰랐어요.\n",
      ">> 입력하신 내용에서 놀람이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 안녕하세요, 반갑습니다!\n",
      ">> 입력하신 내용에서 행복이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 화장품 추천 좀 해주세요\n",
      ">> 입력하신 내용에서 중립이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이런 일이 벌어질 줄이야... 너무 속상해요\n",
      ">> 입력하신 내용에서 슬픔이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 도와주세요! 긴급 상황입니다!\n",
      ">> 입력하신 내용에서 공포가 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 잘 지내셨나요?\n",
      ">> 입력하신 내용에서 놀람이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 잘 지내셨어요?\n",
      ">> 입력하신 내용에서 놀람이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 화가 나서 너무 화가 나요!\n",
      ">> 입력하신 내용에서 분노가 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이 영화 정말 재미있어요!\n",
      ">> 입력하신 내용에서 행복이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이 사진 정말 예쁘네요. 감사합니다!\n",
      ">> 입력하신 내용에서 행복이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 어떤 책을 추천해주시나요?\n",
      ">> 입력하신 내용에서 중립이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 이 신제품은 너무 신기해요!\n",
      ">> 입력하신 내용에서 놀람이 느껴집니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 0\n"
     ]
    }
   ],
   "source": [
    "#질문 무한반복하기! 0 입력시 종료\n",
    "end = 1\n",
    "while end == 1 :\n",
    "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
    "    if sentence == \"0\" :\n",
    "        break\n",
    "    predict(sentence)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
