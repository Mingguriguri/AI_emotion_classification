# AI_emotion_classification
[ì¸ê³µì§€ëŠ¥] ê°ì • ë¶„ì„ê¸° with. Chat GPT

> [íŒ€ê³¼ì œ] ChatGPTì˜ ì½”ë”© ìš”ì²­í•˜ì—¬ ì½”ë”© ìˆ˜ì¤€ì„ í‰ê°€í•œ í›„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì—¬ ì œì¶œ (2023.5.24)
íŒ€ì—ì„œ í”„ë¡œê·¸ë¨ í•˜ë‚˜ë¥¼ ì •í•´ ì±—GPTì—ê²Œ ìš”ì²­í•œ í›„, ì½”ë”© ìˆ˜ì¤€ì„ í‰ê°€í•œ í›„, ë³´ê³ ì„œë¡œ ì‘ì„±í•˜ì—¬ ì œì¶œ
íŒ€ì—ì„œ ì„ ì •í•œ í”„ë¡œê·¸ë¨ì„ chatGPTë”ëŸ¬ ë§Œë“¤ì–´ ë‹¬ë¼ê³  í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ê°€ì§€ê³  ì—¬ëŸ¬ê°€ì§€ ì‹œí—˜ì„ í•´ë³´ë©´ ë©ë‹ˆë‹¤.
ì˜¬ë°”ë¥¸ outputì„ ë°›ì•„ë„, "í‹€ë ¸ë‹¤. ë‹¤ì‹œ í•´ë‹¬ë¼"ê³  í•´ë³´ê³ , ë¯¸í¡í•œ ì ì´ ìˆìœ¼ë©´ ê°€ì´ë“œë¥¼ í•˜ì—¬ ì œëŒ€ë¡œ ëœ ê²ƒì„ ë§Œë“¤ë„ë¡ í•˜ê³ , source codeë§Œ ë§ê³ , flow graph, pseudo code,  test caseë„ ë‹¬ë¼ê³  í•´ë³´ê³ ,í•™ìƒì´ ì‘ì„±í•œ codeë¥¼ ì£¼ë©´ì„œ, ë¬´ì—‡ì´ ì˜ëª»ë˜ì—ˆë˜ì§€ ë¯¸í¡í•œì§€ ë¬¼ì–´ ë³´ëŠ” ë“±ì…ë‹ˆë‹¤.
ë§ˆì§€ë§‰ìœ¼ë¡œ, chatGPTê°€ ê°€ì§€ê³  ì˜¬ ê¸ì •ì /ë¶€ì •ì  ì˜í–¥ì— ëŒ€í•´ ìƒê°í•´ ë³´ê³ ,
ì´ê²ƒì´ êµìœ¡, ì¸ë¬¸ê³„ì—´, entertainment (ì›¹íˆ°, ì˜í™”, ê³µì—° ë“±), ë¬¸í•™, ì˜ˆìˆ  ë“±ì— ë¯¸ì¹  ì˜í–¥ì— ëŒ€í•´ ìƒê°ì„ ì •ë¦¬í•´ì„œ ì œì¶œí•˜ë©´ ë©ë‹ˆë‹¤.

---
# ê°ì • ë¶„ë¥˜ í”„ë¡œê·¸ë¨

> ì—­í•  ë¶„ë‹´
- ê¹€ë¯¼ìˆ˜: chatGPT ìˆ˜ì¤€ í‰ê°€
- ê¹€ë¯¼ì •(ME): ê°ì • ë¶„ì„ê¸° ì„¤ëª…, ê°ì • ë¶„ë¥˜ ëª¨ë¸ êµ¬í˜„ ë° ì£¼ì„, ì„¤ëª… ì‘ì„±
- ê¹€í™ì£¼: chatGPT ìˆ˜ì¤€ í‰ê°€
- ê¹€ë„ì—°: ë¬¸ì„œí™”, chatGPT ì˜í–¥ ë¶„ì„

---
> **ì•„ë˜ ë‚´ìš©ì€ ë‚´ê°€ íŒ€ì›ë“¤ì—ê²Œ 'íŒ€ì›ë“¤ì˜ ì´í•´ ë° ë ˆí¬íŠ¸ ì‘ì„± ìë£Œ'ë¡œì¨ ê³µìœ í•œ md íŒŒì¼ì…ë‹ˆë‹¤. ì œê°€ ë‹´ë‹¹í•œ ë¶€ë¶„ì— ëŒ€í•´ì„œë§Œ ìˆë‹¤ëŠ” ì  ì°¸ê³ í•´ì£¼ì„¸ìš”**
- ìë£Œì¡°ì‚¬ ê¸°ê°„: 2023.5.6 ~ 2023.5.10
- ì‹¤ì œ ëª¨ë¸ êµ¬í˜„ ê¸°ê°„: 2023.5.6 ~ 2023.5.21

# ìì—°ì–´ì²˜ë¦¬(Natural Languagel Processing; NLP)

- ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´, ìƒì„±, ì¡°ì‘í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼
- ìì—°ì–´ í…ìŠ¤íŠ¸ ë˜ëŠ” ìŒì„±ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒí˜¸ ì—°ê²°

## Large Language Model

- Large Language Model(LLM) : ì–¸ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•´ í•™ìŠµëœ ë”¥ëŸ¬ë‹ ëª¨ë¸
    - ëŒ€í™”í˜• ì±—ë´‡, ë²ˆì—­, ë¬¸ì„œ ìƒì„± ë“± ë‹¤ì–‘í•œ NLP(Natural Language Processing)
- íŠ¹ì • ì‘ì—…ì´ë‚˜ ë„ë©”ì¸ì— ë§ê²Œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ Fine-tuningì´ í•„ìš”í•¨

## Transfer Learning(ì „ì´í•™ìŠµ)


>ğŸ“Œ **í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ë¶„ì•¼ì˜ ëª¨ë¸ êµ¬ì¶•ì„ ìœ„í•´ ë°ì´í„°ê°€ í’ë¶€í•œ ë¶„ì•¼ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ëŠ” í•™ìŠµ ê¸°ë²•**

- ë°ì´í„° ìˆ˜ê°€ ë§ì§€ ì•Šê±°ë‚˜ ë°ì´í„°ë¥¼ í™•ë³´í•˜ëŠ”ë° ë§ì€ ë¹„ìš©ì´ ë“¤ ìˆ˜ ìˆìŒ. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•¨
- íŠ¹ì • ë¶„ì•¼ì—ì„œ í•™ìŠµëœ ì‹ ê²½ë§ì˜ ì¼ë¶€ ëŠ¥ë ¥ì„ ìœ ì‚¬í•˜ê±°ë‚˜ ì „í˜€ ìƒˆë¡œìš´ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì‹ ê²½ë§ì˜ í•™ìŠµì— ì´ìš©í•˜ëŠ” ë°©ë²•
- ë”°ë¼ì„œ ê¸°ì¡´ì˜ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ëª¨ë¸ì„ ë§Œë“¤ì‹œ í•™ìŠµì„ ë¹ ë¥´ê²Œ í•˜ì—¬, ì˜ˆì¸¡ì„ ë” ë†’ì„
- Pretrained Model :ì „ì´í•™ìŠµì—ì„œ ì´ìš©ë˜ëŠ” í•™ìŠµëœ ì‹ ê²½ë§
    - **ex. ImageNet, ResNet, gooGleNet, VGGNet**
- ëŒ€ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì˜ í›ˆë ¨ëœ Pretrained Modelì„ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ìê°€ ì ìš©í•˜ë ¤ê³  í•˜ëŠ” ë¬¸ì œ ìƒí™©ì— ë§ê²Œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì•½ê°„ì”© ë³€í™”í•˜ì—¬ ì‚¬ìš©í•¨.
    - (ì‹¤ì œë¡œ, CNNì„ ì´ìš©í•˜ëŠ” ê²½ìš°ì— ì²˜ìŒë¶€í„° ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒë³´ë‹¨ pretrained modelì„ ì‚¬ìš©í•˜ì—¬ ì–´ëŠì €ë„ í•©ë‹¹í•œ ê°’ì„ ê°€ì§„ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•œë‹¤.
- ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì¬ì •ì˜í–ˆë‹¤ë©´, pretrained modelì˜ classfierëŠ” ì‚­ì œí•˜ê³ , ëª©ì ì— ë§ëŠ” ìƒˆë¡œìš´ classifierë¥¼ ì¶”ê°€í•¨. ì´ë ‡ê²Œ ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ Fine Tuningì„ ì§„í–‰í•¨

### Q. ì™œ Transfer Learningì„ ì‚¬ìš©í•˜ëŠ”ê°€?

1. ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ í•´ê²°í•¨
2. ì´ë¯¸ í•™ìŠµëœ ë§ì€ ëª¨ë¸ì€ ì ìš©í•˜ë ¤ëŠ” ë°ì´í„°ê°€ í•™ìŠµí•  ë•Œì˜ ë°ì´í„°ì™€ ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì • í–ˆì„ ë•Œ íš¨ìœ¨ì . 
3. ìƒˆë¡œìš´ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë°”ë€Œë©´ ê¸°ì „ì˜ í†µê³„ì  ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ë‹¤ì‹œ ë§Œë“¤ì–´ì•¼ í•  ë•Œ ì¢‹ìŒ
4. ë³µì¡í•œ ëª¨ë¸ì¼ ë•Œ í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆê³ , í•™ìŠµì‹œí‚¤ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŒ.
5. layerì˜ ê°œìˆ˜, Activation function, Hyper-parametersë“± ëª¨ë¸ì„ êµ¬ì„±í•˜ëŠ”ë° ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ì´ ë§ìœ¼ë©° ì§ì ‘ ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ë§ì€ ì‹œë„ê°€ í•„ìš”í•¨.

**â‡’ ì´ëŸ¬í•œ ì´ìœ ë“¤ë¡œ ì¸í•´ ì´ë¯¸ ì˜ í›ˆë ¨ëœ ëª¨ë¸ì´ ìˆê³ , ë§Œë“œë ¤ëŠ” ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ëª¨ë¸ì¼ ê²½ìš° Transfer Learning(ì „ì´í•™ìŠµ) ì‚¬ìš©**


>ğŸ“Œ **ì´ë•Œ ì‚¬ìš©ë°©ë²• : Fine-Tuning**

## Fine Tuning (íŒŒì¸ íŠœë‹)

>ğŸ“Œ **ê¸°ì¡´ì— í•™ìŠµë˜ì–´ì ¸ ìˆëŠ” ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„í‚¤í…ì³ë¥¼ ìƒˆë¡œìš´ ëª©ì ì— ë§ê²Œ ë³€í˜•í•˜ê³  
ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ Weightsë¡œ ë¶€í„° í•™ìŠµì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•**


- ì‚¬ì „ì— í•™ìŠµëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ì‘ì—…ì— ëŒ€í•œ ì¶”ê°€ í•™ìŠµì„ ìˆ˜í–‰í•˜ì—¬, ì•„í‚¤í…ì²˜ë¥¼ ìƒˆë¡œìš´ ëª©ì ì— ë§ê²Œ ë³€í˜•í•˜ê³  ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¯¸ì„¸í•˜ê²Œ ì¡°ì •í•˜ì—¬ í•™ìŠµ ì‹œí‚¤ëŠ” ë°©ë²•
    - ex. ì˜ë£Œë¶„ì•¼ ì±—ë´‡ì„ ë§Œë“¤ê¸° ìœ„í•´ ì¼ë°˜ì ì¸ ì–¸ì–´ ë°ì´í„°ì…‹ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ë£Œ ê´€ë ¨ ì–¸ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ê°€ í•™ìŠµì´ í•„ìš”í•¨
        
        â‡’ ë”°ë¼ì„œí•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ë°ì´í„°ì…‹ì´ í•„ìš”í•¨.
        

## â­ì‚¬ìš©í•œ ëª¨ë¸ : BERT, KoBERT

- **BERT**
    - **êµ¬ê¸€**ì—ì„œ 2018ë…„ì— ê³µê°œë¨
    - GPT ë“± ë‹¤ë¥¸ ëª¨ë¸ê³¼ëŠ” ë‹¤ë¥´ê²Œ **ì–‘ë°©í–¥ì„±**ì„ ì§€í–¥í•˜ê¸° ë•Œë¬¸ì—, ìˆ˜ë§ì€ **NLP**ì˜ í•œ íšì„ ê·¸ì€ ëª¨ë¸ë¡œ í‰ê°€ë°›ê³  ìˆìŒ
    - **ë¬¸ë§¥ íŠ¹ì„±**ì„ í™œìš©í•˜ê³  ìˆê³ , **ëŒ€ìš©ëŸ‰ ë§ë­‰ì¹˜**ë¡œ ì‚¬ì „ í•™ìŠµì´ ì´ë¯¸ ì§„í–‰ë˜ì–´ ì–¸ì–´ì— ëŒ€í•œ ì´í•´ë„ê°€ ë†’ìŒ
    - í•˜ì§€ë§Œ BERTëŠ” **í•œêµ­ì–´ì— ëŒ€í•´ì„œ ì˜ì–´ë³´ë‹¤ ì •í™•ë„ê°€ ë–¨ì–´ì§**
    
- **KoBERT(Korean BERT)**
    - SKTBrainì—ì„œ ê³µê°œí•¨
    - **í•œêµ­ì–´ ìœ„í‚¤ 5ë°±ë§Œ ë¬¸ì¥ê³¼ í•œêµ­ì–´ ë‰´ìŠ¤ 2ì²œë§Œ ë¬¸ì¥ì„ í•™ìŠµí•œ ëª¨ë¸**
    - BERTëª¨ë¸ ì¤‘ì—ì„œ **KoBERTë¥¼ ì‚¬ìš©í•œ ì´ìœ ** : **â€œí•œêµ­ì–´â€**ì— ëŒ€í•´ ë§ì€ ì‚¬ì „ í•™ìŠµì´ ì´ë£¨ì–´ì ¸ìˆê³ , ê°ì • ë¶„ì„í•  ë•Œ ê¸ì • ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ë‹¤ì¤‘ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸
    - ì´ëŠ” ìì‹ ì˜ ëª©ì ì— ë”°ë¼ì„œ, **ì„¸ë¶€ì¡°ì •(Fine-tuning)ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— output lyaerë§Œ ì¶”ê°€ë¡œ ë‹¬ì•„ì£¼ë©´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì¶œë ¥í•´ë‚¼ ìˆ˜ ìˆìŒ**

# ì½”ë“œ ì„¤ëª…

## 0. ë°ì´í„°ì…‹ ì„¤ëª… / ì½”ë”© í™˜ê²½

- ë„¤ì´ë²„ ë¦¬ë·° 2ì¤‘ ë¶„ë¥˜ ì˜ˆì œ ì½”ë“œ ê¸°ë°˜ ([https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb](https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb))
- AiHubì˜ ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ëŒ€í™” ìŒì„± ë°ì´í„°ì…‹ ì´ìš©
    - 7ê°€ì§€ì˜ ê°ì •ì´ ìˆìŒ
        - í–‰ë³µ, ë¶„ë…¸, í˜ì˜¤, ê³µí¬, ì¤‘ë¦½, ìŠ¬í””, ë†€ëŒ
- ì½”ë©ì—ì„œëŠ” ë²„ì „ ì¶©ëŒì´ ë°œìƒí•˜ì—¬, **ì£¼í”¼í„° ë…¸íŠ¸ë¶**ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë”©í•˜ê³  í…ŒìŠ¤íŠ¸í–ˆìŒ

(ë§Œë“  íŒŒì¼ì€ ë”°ë¡œ ì°¸ê³ í•˜ëŠ” ê²Œ ì¢‹ì„ ë“¯!)

## 1. í™˜ê²½ ì„¤ì •(ğŸš¨Errorë°œìƒ)
![1](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/5f6e4bee-e290-426f-9b20-15ce6f1ca0b0)

![2](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/768b0ca3-4886-4db4-8497-005dbb5b6c70)

KoBERTê°€ ìš”êµ¬í•˜ëŠ” ìµœì‹  ì •ë³´ë¥¼ í† ëŒ€ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œë‹¤. 

```python
!pip install mxnet
!pip install gluonnlp==0.8.0
!pip install pandas tqdm   
!pip install sentencepiece==0.1.91
!pip install transformers==4.8.2
!pip install torch
```

- `mxnet`ì´ ìš°ì„  ì„¤ì¹˜ë˜ì–´ì•¼ í•˜ë©°, `gluonnlp`ëŠ” 0.8.0ìœ¼ë¡œ ì„¤ì •í•œë‹¤.

## 2. githubì—ì„œ KoBERT íŒŒì¼ì„ ë¡œë“œí•˜ê³  KoBERTëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê¸°

- ê¹ƒí—ˆë¸Œ íŒŒì¼ì˜ kobert_tokenizerí´ë”ë¥¼ ë‹¤ìš´ë°›ëŠ”ë‹¤.
![3](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/7f58a279-ffc2-497d-bf30-b9394f59d5dd)

    ```python
    !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'
    ```

    
- ë¶ˆëŸ¬ì˜¨ í›„, ìš°ë¦¬ê°€ í•„ìš”í•œ tokenizerì™€ model, vocabularyë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.
    - tokenizer : `tokenizer`
    - model : `bertmodel`
    - vocabulary: `vocab`
      
    ![4](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/5bd59275-866b-4cec-b81e-9328cf103763)

    ```python
    from kobert_tokenizer import KoBERTTokenizer # í•œêµ­ì–´ BERT ëª¨ë¸ì— ëŒ€í•œ tokenizerë¥¼ import
    from transformers import BertModel # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ BertModelì„ import
    
    tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1') # pretrainedëœ 'skt/kobert-base-v1' ëª¨ë¸ë¡œ KoBERTTokenizerë¥¼ ì´ˆê¸°í™”
    bertmodel = BertModel.from_pretrained('skt/kobert-base-v1') # pretrainedëœ 'skt/kobert-base-v1' ëª¨ë¸ë¡œ BertModelì„ ì´ˆê¸°í™”
    vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]') # tokenizerì˜ vocab_fileì„ ì‚¬ìš©í•˜ì—¬ BERTVocabì„ ìƒì„±.padding í† í°ì€ '[PAD]'ë¡œ ì„¤ì • 
    
    #model, vocab = BertModel.from_pretrained('skt/kobert-base-v1', tokenizer.vocab_file)
    
    print(vocab)
    ```
    
    - ì´ ë¶€ë¶„ì—ì„œ êµ‰ì¥íˆ ë§ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤.
    - ë¸”ë¡œê·¸ì— ë‚˜ì™€ìˆëŠ” ê¹ƒí—ˆë¸Œ ë°©ë²•ì´ ì•„ë‹Œ **hugging face**ë¥¼ í†µí•´ ê°€ì ¸ì˜¨ë‹¤.
    

## 3. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (ğŸš¨Errorë°œìƒ)

ì‚¬ì „ í•™ìŠµëœ(pre-trained) ëª¨ë¸ì¸ BERTë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” transformersë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ìì£¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í˜¸ì¶œí•˜ì˜€ë‹¤.

ë˜í•œ, í•™ìŠµì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ GPUë¥¼ ì‚¬ìš©í•œë‹¤.


```python
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook
import pandas as pd

#transformers
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup
from transformers import BertModel

#GPU ì‚¬ìš© ì‹œ
device = torch.device("cuda:0")
```

## 4. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

ì´ì „ì— AiHubì— ê¸°ì¬ë˜ì—ˆë˜ ë°ì´í„°(í˜„ì¬ëŠ” ì—†ìŒ)ë¥¼ ì˜¬ë ¤ì¤€ ë¸”ë¡œê·¸ë¥¼ í†µí•´ì„œ ë‹¤ìš´ ë°›ì•˜ë‹¤.


![5](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/4c8aae58-eebb-446d-87b5-4b404342dfab)

```python
data = pd.read_excel('/home/jihwan/MinJeong Archive/ê°ì •ë¶„ë¥˜ë°ì´í„°ì…‹.xlsx')
data.sample(n=10)
```
![Untitled](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/6eb7d4c1-6400-4109-994b-d27ed87dc563)


í•´ë‹¹ ë°ì´í„° ì…‹ì— ëŒ€í•´ì„œ ì´ 7ê°œì˜ ê°ì • classë¥¼ 0~6ê°œì˜ ìˆ«ì(label)ì— ëŒ€ì‘ì‹œì¼œ data_listì— ë‹´ì•„ì¤€ë‹¤.
![6](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/0fd1ccbb-d68d-4909-899c-0b2492dd9cdf)

```python
#locë¥¼ ì‚¬ìš©í•˜ì—¬ 'Emotion' ì—´ì˜ ê°’ì´ íŠ¹ì • ë¬¸ìì—´ê³¼ ì¼ì¹˜í•˜ëŠ” í–‰ì„ ì°¾ê³ , í•´ë‹¹ í–‰ë“¤ì˜ 'Emotion' ì—´ ê°’ì„ ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰
data.loc[(data['Emotion'] == "ê³µí¬"), 'Emotion'] = 0  #ê³µí¬ => 0
data.loc[(data['Emotion'] == "ë†€ëŒ"), 'Emotion'] = 1  #ë†€ëŒ => 1
data.loc[(data['Emotion'] == "ë¶„ë…¸"), 'Emotion'] = 2  #ë¶„ë…¸ => 2
data.loc[(data['Emotion'] == "ìŠ¬í””"), 'Emotion'] = 3  #ìŠ¬í”” => 3
data.loc[(data['Emotion'] == "ì¤‘ë¦½"), 'Emotion'] = 4  #ì¤‘ë¦½ => 4
data.loc[(data['Emotion'] == "í–‰ë³µ"), 'Emotion'] = 5  #í–‰ë³µ => 5
data.loc[(data['Emotion'] == "í˜ì˜¤"), 'Emotion'] = 6  #í˜ì˜¤ => 6

data_list = []

# ë°ì´í„°í”„ë ˆì„ì˜ 'Sentence'ê³¼ 'Emotion' columnì„ ëŒë©´ì„œ
for ques, label in zip(data['Sentence'], data['Emotion'])  :
    data = []   #  ê° ë°˜ë³µì—ì„œ ìƒˆë¡œìš´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
    data.append(ques) # ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥(ques)ì™€ ë³€í™˜ëœ ê°ì •ë ˆì´ë¸”ì„ ì¶”ê°€
    data.append(str(label)) #ê°ì • ë ˆì´ë¸”ì„ ë¬¸ìì—´ëŸ¬ ë³€í™˜

    data_list.append(data) #ì²˜ë¦¬í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ì²´ ë°ì´í„°ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
```

## 5. ì…ë ¥ ë°ì´í„°ì…‹ì„ í† í°í™”í•˜ê¸° (ğŸš¨Errorë°œìƒ)

ê° ë°ì´í„°ê°€ BERT ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë„ë¡Â **tokenization, int encoding, padding**Â ë“±ì„ í•´ì£¼ëŠ” ì½”ë“œì´ë‹¤.


```python
class BERTDataset(Dataset):

	# ì´ˆê¸°í™” í•¨ìˆ˜, Dataset ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ì‹¤í–‰
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,
                 pad, pair):
				# BERT ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë¬¸ì¥ì„ ë³€í™˜í•˜ëŠ” transform í•¨ìˆ˜ë¥¼ ìƒì„±
        # BERTSentenceTransform í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ë¬¸ì¥ì„ BERT ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•¨
				# í•´ë‹¹ í•¨ìˆ˜ë¶€ë¶„ì—ì„œ ì—ëŸ¬ ë°œìƒ
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)
        self.sentences = [transform([i[sent_idx]]) for i in dataset] # ì…ë ¥ë°›ì€ datasetì—ì„œ ê° ë¬¸ì¥(sent_idxì— í•´ë‹¹í•˜ëŠ” í•­ëª©)ì„ BERT ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•˜ê³  ì €ì¥
        self.labels = [np.int32(i[label_idx]) for i in dataset] # ì…ë ¥ë°›ì€ datasetì—ì„œ ê° ë¬¸ì¥ì˜ ë ˆì´ë¸”(label_idxì— í•´ë‹¹í•˜ëŠ” í•­ëª©)ì„ ì €ì¥

		# iì— í•´ë‹¹í•´ëŠ” ìƒ˜í”Œì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜, ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œ ì‚¬ìš©
    def __getitem__(self, i): 
        return (self.sentences[i] + (self.labels[i], ))
         
		# ì´ ìƒ˜í”Œì˜ ìˆ˜(ë°ì´í„°ì…‹ì˜ ê¸¸ì´) ë°˜í™˜ í•¨ìˆ˜
    def __len__(self):
				return len(self.sentences)
```

- **ì—ëŸ¬ ë°œìƒ ë¶€ë¶„**
    - ì£¼ì„ìœ¼ë¡œ í‘œì‹œí•´ë†“ì€ ë¶€ë¶„, `nlp.data.BERTSentenceTransform`ì€ ìœ„ì—ì„œ ì—ëŸ¬ê°€ ë‚¬ë˜, `import gluonnlp as nlp`ì—ì„œì™€ ê°™ì€ íŒ¨í‚¤ì§€ì´ë‹¤.
    - ì´ ë¶€ë¶„ì—ì„œëŠ” íŒŒì´ì¬ì´  ì—…ê·¸ë ˆì´ë“œë˜ë©´ì„œ `gluonnlp`ì™€ëŠ” í˜¸í™˜ì´ ë˜ì§€ ì•Šìœ¼ë©´ì„œ ë¬¸ì œê°€ ë°œìƒí•œ ê²ƒì´ë©°, ì •í™•íˆëŠ” `BERTSentenceTransform`ì—ì„œ ë‚œ ì—ëŸ¬ì´ë‹¤.
    - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´  ì—¬ëŸ¬ ìë£Œë¥¼ ì°¾ì•„ë³´ì•˜ìœ¼ë©° ê¹ƒí—ˆë¸Œë„ ì°¾ì•„ë³´ì•˜ì§€ë§Œ, ê¹ƒí—ˆë¸Œì—ë„ ì—…ê·¸ë ˆì´ë“œëœ ì‚¬í•­ì´ ë‚˜ì™€ìˆì§€ ì•Šì•˜ë‹¤.
    - ì´í›„, ë‹¤ë¥¸ ì‚¬ëŒì´ ì§ì ‘ `BERTSentenceTransform`ì„ ì§ì ‘ ìƒˆë¡œ êµ¬í˜„í•˜ì‹  ë¶„ì˜ ìë£Œë¥¼ ë°œê²¬í•˜ì—¬ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì˜€ë‹¤. ([https://blog.naver.com/newyearchive/223097878715](https://blog.naver.com/newyearchive/223097878715))
        - pyíŒŒì¼ì—ì„œ classë¥¼ ë³µë¶™í•˜ì—¬ ì½”ë“œë¥¼ ìˆ˜ì •í•œë‹¤.
        - def __init__ì—ì„œ inputì— vocabë¥¼ ë°›ëŠ” ë¶€ë¶„ ì¶”ê°€,  self._vocab = vocab ì„ ì¶”ê°€í•˜ê³ ,def __call__ì—ì„œ vocab = self._vocabë¡œ ë°”ê¿”ì£¼ì…¨ë‹¤.

BERT ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤ì¸ **`BERTSentenceTransform`**ë¥¼ ì¬ì •ì˜

![7(1)](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/204dac3d-3d5c-4b4a-a9db-934a8ae91f33)
![7(2)](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/9e1d28c4-045e-4b54-a1e5-0a15553ac6c4)


```python
class BERTSentenceTransform:
    """BERT style data transformation.

    Parameters
    ----------
    tokenizer : BERTTokenizer.
        Tokenizer for the sentences.
    max_seq_length : int.
        Maximum sequence length of the sentences.
    pad : bool, default True
        Whether to pad the sentences to maximum length.
    pair : bool, default True
        Whether to transform sentences or sentence pairs.
    """
		# ì´ˆê¸°í™” ë©”ì„œë“œ, BERTSentenceTransform ê°ì²´ê°€ ìƒì„±ë  ë•Œ ì‹¤í–‰
    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):
        self._tokenizer = tokenizer
        self._max_seq_length = max_seq_length
        self._pad = pad
        self._pair = pair
        self._vocab = vocab 

		# ê°ì²´ê°€ í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œë  ë•Œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜; bertëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë¬¸ì¥ì„ ë³€í™˜í•¨
    def __call__(self, line):
        # ì…ë ¥ëœ ë¬¸ì¥ì„ unicode í˜•íƒœë¡œ ë³€í™˜
        text_a = line[0]
        if self._pair:
            assert len(line) == 2
            text_b = line[1]

				# ë¬¸ì¥ì„ BERT ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í°ìœ¼ë¡œ ë¶„ë¦¬
        tokens_a = self._tokenizer.tokenize(text_a)
        tokens_b = None

        if self._pair:
            tokens_b = self._tokenizer(text_b)

				# ì—¬ê¸°ì—ì„œ tokens_aì™€ tokens_bëŠ” BERT ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í† í°

        if tokens_b:
            # ì´ í† í°ë“¤ì˜ ê¸¸ì´ê°€ max_seq_lengthë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì¡°ì •
		        # ì´ë•Œ [CLS], [SEP], [SEP] í† í°ì˜ ìë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ "-3"
						# ê° ì‹œí€€ìŠ¤ëŠ” [CLS], [SEP] í† í°ì„ í¬í•¨í•´ì•¼ í•˜ë©°, ë‘ ì‹œí€€ìŠ¤ë¥¼ ì—°ê²°í•  ë•ŒëŠ” ë‘ ë²ˆì§¸ [SEP] í† í°ì´ í•„ìš”í•˜ë¯€ë¡œ, ì´ ì„¸ ê°œì˜ í† í°ì— ëŒ€í•œ ê³µê°„ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ 3ì„ ëºŒ
						# ì¦‰, ì „ì²´ í† í°ì˜ ê°œìˆ˜ê°€ [CLS], tokens_a, [SEP], tokens_b, [SEP]ë¥¼ í¬í•¨í•œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ tokens_aì™€ tokens_bì˜ ê¸¸ì´ë¥¼ ì¡°ì •
            self._truncate_seq_pair(tokens_a, tokens_b,
                                    self._max_seq_length - 3)
        else:
            # ë§Œì•½ tokens_bê°€ ì—†ë‹¤ë©´, [CLS], [SEP] í† í°ì˜ ìë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•´ "-2"ì„ í•©ë‹ˆë‹¤.
            if len(tokens_a) > self._max_seq_length - 2:
                tokens_a = tokens_a[0:(self._max_seq_length - 2)]

        # The embedding vectors for `type=0` and `type=1` were learned during
        # pre-training and are added to the wordpiece embedding vector
        # (and position vector). This is not *strictly* necessary since
        # the [SEP] token unambiguously separates the sequences, but it makes
        # it easier for the model to learn the concept of sequences.

        # For classification tasks, the first vector (corresponding to [CLS]) is
        # used as as the "sentence vector". Note that this only makes sense because
        # the entire model is fine-tuned

        vocab = self._vocab #vocab = self._tokenizer.vocab
        tokens = []
        tokens.append(vocab.cls_token)
        tokens.extend(tokens_a)
        tokens.append(vocab.sep_token)
        segment_ids = [0] * len(tokens) 

        if tokens_b:
            tokens.extend(tokens_b)
            tokens.append(vocab.sep_token)
            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))

        input_ids = self._tokenizer.convert_tokens_to_ids(tokens) # í† í°ì„ í•´ë‹¹í•˜ëŠ” idë¡œ ë³€í™˜

        valid_length = len(input_ids) # ì‹¤ì œ í† í°ì˜ ê¸¸ì´ë¥¼ ì €ì¥

				# pad ì˜µì…˜ì´ Trueì¸ ê²½ìš°, ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ max_seq_lengthë¡œ ë§ì¶°ì£¼ê¸° ìœ„í•´ paddingì„ ì¶”ê°€
        if self._pad:
            # Zero-pad up to the sequence length.
            padding_length = self._max_seq_length - valid_length
            # use padding tokens for the rest
            input_ids.extend([vocab[vocab.padding_token]] * padding_length)
            segment_ids.extend([0] * padding_length)
				

				# ê²°ê³¼ ë°˜í™˜
        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\
            np.array(segment_ids, dtype='int32')
```

ì´í›„ BERTDatasetì„ ë‹¤ì‹œ ì •ì˜í•œë‹¤. í•´ë‹¹ ë¶€ë¶„ì€ ìœ„ì™€ 

![8](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/03d2de05-915d-49cc-9ed4-b53370fa7065)

```python
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertModel
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

class BERTDataset(Dataset):

	# ì´ˆê¸°í™” í•¨ìˆ˜, Dataset ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ì‹¤í–‰
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,
                 pad, pair):

				# BERT ëª¨ë¸ì— ì í•©í•œ í˜•íƒœë¡œ ë¬¸ì¥ì„ ë³€í™˜í•˜ëŠ” transform í•¨ìˆ˜ë¥¼ ìƒì„±
        # BERTSentenceTransform í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ë¬¸ì¥ì„ BERT ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•¨
				# í•´ë‹¹ í•¨ìˆ˜ë¶€ë¶„ì—ì„œ ì—ëŸ¬ ë°œìƒ
        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)
        #transform = nlp.data.BERTSentenceTransform(
        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)
        self.sentences = [transform([i[sent_idx]]) for i in dataset] # ì…ë ¥ë°›ì€ datasetì—ì„œ ê° ë¬¸ì¥(sent_idxì— í•´ë‹¹í•˜ëŠ” í•­ëª©)ì„ BERT ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•˜ê³  ì €ì¥
        self.labels = [np.int32(i[label_idx]) for i in dataset]  # ì…ë ¥ë°›ì€ datasetì—ì„œ ê° ë¬¸ì¥ì˜ ë ˆì´ë¸”(label_idxì— í•´ë‹¹í•˜ëŠ” í•­ëª©)ì„ ì €ì¥

		# iì— í•´ë‹¹í•´ëŠ” ìƒ˜í”Œì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜, ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œ ì‚¬ìš©
    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))
		
	# ì´ ìƒ˜í”Œì˜ ìˆ˜(ë°ì´í„°ì…‹ì˜ ê¸¸ì´) ë°˜í™˜ í•¨ìˆ˜
    def __len__(self):
        return (len(self.labels))
```

- **`__init__`** í•¨ìˆ˜
    - BERTDataset ê°ì²´ê°€ ìƒì„±ë  ë•Œ ì‹¤í–‰ë˜ë©°, BERT ëª¨ë¸ì— í•„ìš”í•œ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ã…£ëŠ” í•¨ìˆ˜
    - **`transform`** í•¨ìˆ˜ë¥¼ í†µí•´ ê° ë¬¸ì¥ì€ BERT ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜ëœë‹¤.
- **`__getitem__`** í•¨ìˆ˜
    - ì¸ë±ìŠ¤ **`i`**ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    - ì´ í•¨ìˆ˜ëŠ” ë°ì´í„° ë¡œë”ì—ì„œ ì‚¬ìš©ë˜ë©°, íŠ¹ì • ì¸ë±ìŠ¤ì˜ ì…ë ¥ ë°ì´í„°ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì„ ë°˜í™˜í•œë‹¤.
- **`__len__`** í•¨ìˆ˜
    - ë°ì´í„°ì…‹ì˜ ì´ ìƒ˜í”Œ ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    - ì´ í•¨ìˆ˜ëŠ” ë°ì´í„° ë¡œë”ê°€ ë°°ì¹˜ë¥¼ ìƒì„±í•  ë•Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•œë‹¤.
    - ì´ë¥¼ í†µí•´ ë°ì´í„° ë¡œë”ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ë§ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.

ì´ë¥¼ í†µí•´ data_trainì—ì„œ `tok`ìœ¼ë¡œ ì„¤ì •í–ˆë˜ ë¶€ë¶„ì„ `tokenizer`ë¡œ ì„¤ì •í•˜ì—¬ ìˆ˜ì •í•˜ì˜€ë‹¤.


```python
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1') #ë¯¸ë¦¬ í›ˆë ¨ëœ 'skt/kobert-base-v1' ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ KoBERT í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False) # ë§ˆì°¬ê°€ì§€ë¡œ 'skt/kobert-base-v1' ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ BertModelì„ ë¡œë“œí•©ë‹ˆë‹¤. 'return_dict=False'ëŠ” ëª¨ë¸ ì¶œë ¥ì„

# í† í¬ë‚˜ì´ì €ì˜ ë‹¨ì–´ì¥ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ BERTVocab ê°ì²´ë¥¼ ìƒì„± 
vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')

data_train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)
data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)
# BERTDataset í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬
```

`parameter`ì˜ ê²½ìš°, ì˜ˆì‹œ ì½”ë“œì— ìˆëŠ” ê°’ë“¤ì„ ë™ì¼í•˜ê²Œ ì„¤ì •í•´ì£¼ì—ˆë‹¤.

![9](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/8421db3b-ee12-479a-a486-0a45334803fb)

```python
# Setting parameters
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 5  
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5
```

ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•´ì£¼ëŠ” `train_test_split` ë©”ì„œë“œë¥¼ í™œìš©í•´ ê¸°ì¡´ `data_list`ë¥¼ **train ë°ì´í„°ì…‹ê³¼ test ë°ì´í„°ì…‹**ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. 5:1 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ì—ˆë‹¤.

![10](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/dd49f317-71fc-44cc-9a2d-86e313fab966)

```python
#train & test ë°ì´í„°ë¡œ ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34)
```

ìœ„ì—ì„œ êµ¬í˜„í•œÂ `BERTDataset` í´ë˜ìŠ¤ë¥¼ í™œìš©í•´ **tokenization,Â int encoding, padding** ì„ ì§„í–‰í•˜ì˜€ë‹¤.
![11](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/7af85489-48b1-4fd5-b32e-acb2a229c836)
<<!!!!!!!!!>>
```python
train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)
test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)
```

torch í˜•ì‹ì˜ datasetì„ ë§Œë“¤ì–´ì£¼ë©´ì„œ, ì…ë ¥ ë°ì´í„°ì…‹ì˜ ì²˜ë¦¬ê°€ ëª¨ë‘ ëë‚¬ë‹¤.

## 6. KoBERT ëª¨ë¸ êµ¬í˜„í•˜ê¸°
![12](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/e3969ffe-f240-4f42-a7dd-f5647fb4bb4c)

```python
class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,  # BERT ëª¨ë¸ì˜ hidden layer í¬ê¸°
                 num_classes=7,  # ë¶„ë¥˜í•  í´ë˜ìŠ¤ì˜ ê°œìˆ˜
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__() 
        self.bert = bert
        self.dr_rate = dr_rate
        # ì¶œë ¥ ë ˆì´ì–´.
				#BERTì˜ hidden_sizeë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³ , ì¶œë ¥ì€ í´ë˜ìŠ¤ì˜ ìˆ˜ë¡œ ì„¤ì •   
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate) #
    
# BERTì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ë™ì‘í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()
		# ìˆœì „íŒŒ í•¨ìˆ˜ ì •ì˜
    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length) # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
```

BERTëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤. 
![13](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/a1ef00a1-c24e-481c-b2ed-86609a7c4a63)

```python
# BERTClassifier í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ìƒì„±
model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)

#optimizerì™€ schedule ì„¤ì •
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss() # ë‹¤ì¤‘ë¶„ë¥˜ë¥¼ ìœ„í•œ ëŒ€í‘œì ì¸ loss func

t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)

scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

#ì •í™•ë„ ì¸¡ì •ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
    
train_dataloader
```

í•´ë‹¹ ë¶€ë¶„ì€ ì˜ˆì œ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì˜€ë‹¤.

## 7. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê¸°

- KoBERT ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì½”ë“œì´ë‹¤. **epoch**ëŠ” 5ë¡œ ì§€ì •í•˜ì˜€ë‹¤.
- ì•ì„œ ì´ ëª¨ë¸ì—ì„œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ì…ë ¥ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ê³ , íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ ì§€ì •í•˜ì˜€ìœ¼ë¯€ë¡œ ì˜ˆì‹œ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì§„í–‰í•˜ì˜€ë‹¤.
- ì´ ë¶€ë¶„ì—ì„œ ì›ë³¸ ì˜ˆì œ ì½”ë“œì™€ ë‹¤ë¥¸ ì ì€ ì •í™•ë„ê°€ ì´ˆë°˜ì— 17%ë¡œ ë‚˜ì™”ë‹¤ëŠ” ê²ƒì´ì—ˆë‹¤.
- ë”°ë¼ì„œ, ì´ ë¶€ë¶„ì„ ìœ„í•˜ì—¬ ìœ„ì— `tokens_a = self._tokenizer(text_a)`ë¡œ ë˜ì–´ ìˆë˜ ë¶€ë¶„ì„ `tokens_a = self._tokenizer.tokenize(text_a)`ë¡œ ìˆ˜ì •í•¨ìœ¼ë¡œì¨ ì •í™•ë„ë¥¼ ë†’ì˜€ë‹¤.

![14(1)](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/a66d4a00-0f53-4eb9-85df-cd8591b7b447)

```python
train_history = []  # í›ˆë ¨ ì •í™•ë„ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
test_history = []  # í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
loss_history = []  # ì†ì‹¤ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸

# ì§€ì •ëœ ì—í­ ìˆ˜ë§Œí¼ ë°˜ë³µ
for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •

    # í›ˆë ¨ ë°ì´í„° ë¡œë”ì—ì„œ ë°°ì¹˜ë³„ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í•™ìŠµì„ ìˆ˜í–‰
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length = valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)

        # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ ìˆ˜í–‰
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  
        train_acc += calc_accuracy(out, label)

        # ì¼ì • ì£¼ê¸°ë§ˆë‹¤ í›ˆë ¨ ìƒíƒœë¥¼ ì¶œë ¥í•˜ê³ , í›ˆë ¨ ì •í™•ë„ì™€ ì†ì‹¤ì„ ê¸°ë¡
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e + 1, batch_id + 1, loss.data.cpu().numpy(), train_acc / (batch_id + 1)))
            train_history.append(train_acc / (batch_id + 1))
            loss_history.append(loss.data.cpu().numpy())

    print("epoch {} train acc {}".format(e + 1, train_acc / (batch_id + 1)))

    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”ì—ì„œ ë°°ì¹˜ë³„ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ í‰ê°€
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length = valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)

    print("epoch {} test acc {}".format(e + 1, test_acc / (batch_id + 1)))
    test_history.append(test_acc / (batch_id + 1))
```
![Untitled 1](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/52d33242-4cc9-48de-a10a-a12ea059d24d)

- ìˆ˜ì •í•˜ê³  ë‚˜ë‹ˆ, **train datasetì— ëŒ€í•´ì„œëŠ”Â 0.979, test datasetì— ëŒ€í•´ì„œëŠ”Â 0.918**ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í–ˆë‹¤.

## 8. ì§ì ‘ ì…ë ¥í•œ ìƒˆë¡œìš´ ë¬¸ì¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

ì´ì œ ì§ì ‘ ë¬¸ì¥ì„ ë§Œë“¤ì–´ í•™ìŠµëœ ëª¨ë¸ì´ ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ì˜ í•´ë‚´ëŠ”ì§€Â ì•Œì•„ë³´ê¸° ìœ„í•´ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¤ì¤‘ ë¶„ë¥˜ëœ í´ë˜ìŠ¤ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” **`predict í•¨ìˆ˜`**ë¥¼ êµ¬í˜„í•œë‹¤.

- **`predict`** í•¨ìˆ˜ëŠ” ì˜ˆì¸¡í•  ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ê°ì •ì„ ì˜ˆì¸¡í•˜ê³  ì¶œë ¥í•œë‹¤.
- ì…ë ¥ ë¬¸ì¥ì„ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±í•˜ê³ , ì´ë¥¼ ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œë‹¤.
- ë³€í™˜í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë”ë¥¼ ìƒì„±í•˜ê³ , ëª¨ë¸ì„ evalëª¨ë“œë¡œ ë°”ê¾¼ë‹¤.
- ë°ì´í„° ë¡œë”ì—ì„œ ë°°ì¹˜ë³„ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ì–´ ë ˆì´ë¸”ì„ ìƒì„±í•˜ê³  ì¶œë ¥í•œë‹¤.

![15](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/abb740f7-cafc-450a-8c40-72bbb38354e0)

```python
def predict(predict_sentence):

    data = [predict_sentence, '0']
    dataset_another = [data] 

    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)
    
    model.eval() # ëª¨ë¸ í‰ê°€ëª¨ë“œ

    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length= valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids) # ëª¨ë¸ì— ë¬¸ì¥ì„ ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.

        test_eval=[]
        for i in out:
            logits=i
            logits = logits.detach().cpu().numpy()

						# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì • ë ˆì´ë¸”ì„ ìƒì„±
            if np.argmax(logits) == 0:
                test_eval.append("ê³µí¬ê°€")
            elif np.argmax(logits) == 1:
                test_eval.append("ë†€ëŒì´")
            elif np.argmax(logits) == 2:
                test_eval.append("ë¶„ë…¸ê°€")
            elif np.argmax(logits) == 3:
                test_eval.append("ìŠ¬í””ì´")
            elif np.argmax(logits) == 4:
                test_eval.append("ì¤‘ë¦½ì´")
            elif np.argmax(logits) == 5:
                test_eval.append("í–‰ë³µì´")
            elif np.argmax(logits) == 6:
                test_eval.append("í˜ì˜¤ê°€")

        print(">> ì…ë ¥í•˜ì‹  ë‚´ìš©ì—ì„œ " + test_eval[0] + " ëŠê»´ì§‘ë‹ˆë‹¤.")
```

ì§ˆë¬¸ ë° ì…ë ¥ì„ ë¬´í•œ ë°˜ë³µí•œë‹¤. ì…ë ¥í•˜ë©´ ì…ë ¥í•œ ë‚´ìš©ì— ë”°ë¼ `predict()`ì—ì„œ ê°ì •ì„ ë¶„ë¥˜í•œ ê°’ì„ ì¶œë ¥í•œë‹¤. `0`ì„ ì…ë ¥í•˜ë©´ ë¬´í•œ ë°˜ë³µì´ ì¢…ë£Œëœë‹¤.

![16](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/26673eed-794b-4dd8-8d62-10afd1d8e40e)

```python
#ì§ˆë¬¸ ë¬´í•œë°˜ë³µí•˜ê¸°! 0 ì…ë ¥ì‹œ ì¢…ë£Œ
end = 1
while end == 1 :
    sentence = input("í•˜ê³ ì‹¶ì€ ë§ì„ ì…ë ¥í•´ì£¼ì„¸ìš” : ")
    if sentence == "0" :
        break
    predict(sentence)
    print("\n")
```
![result(2)](https://github.com/Mingguriguri/AI_emotion_classification/assets/101111603/33f8a06a-b7e2-4619-a77d-eca6bfd11b1d)



# ê²°ë¡ 

ìˆ˜ì—…ì‹œê°„ì— ë°°ìš´ ìì—°ì–´ì²˜ë¦¬ë¥¼ ë” ê¹Šê²Œ ë‚˜ì•„ê°€, ê°ì •ë³„ë¡œ ë¶„ë¥˜ë¥¼ í•´ë³´ì•˜ë‹¤. ë°ì´í„°ì…‹ì„ ì§ì ‘ fine tuningí•˜ëŠ” ê³¼ì •ì„ ê±°ì³ ì§ì ‘ ì§ˆë¬¸ì— ë¶„ë¥˜í•˜ëŠ” ê²ƒê¹Œì§€ í•´ë³´ì•˜ë‹¤. 

í›„ì— ì§ì ‘ ë°ì´í„°ì…‹ì„ ì¶”ê°€í•˜ê³ , ë°ì´í„°ë¥¼ ë” ì •ì œí•˜ì—¬ ì •í™•ë„ë„ ë†’ì´ë©´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ë‹¤.

---

ì°¸ê³ ìë£Œ

- [https://hoit1302.tistory.com/159](https://hoit1302.tistory.com/159)
- [https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb](https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb)
- ë²„ê·¸ í•´ê²° ê´€ë ¨í•˜ì—¬ ì°¸ê³ í•œ ê³³ : SKTBrain/KoBERTê¹ƒí—ˆë¸Œ contact í˜ì´ì§€ https://github.com/SKTBrain/KoBERT/issues
- No module named â€˜kobertâ€™ ì—ëŸ¬ í•´ê²°, [https://blog.naver.com/newyearchive/223097878715](https://blog.naver.com/newyearchive/223097878715)
